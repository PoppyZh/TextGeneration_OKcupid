{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textgenrnn import textgenrnn\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = {\n",
    "    'word_level': True,   # set to True if want to train a word-level model (requires more data and smaller max_length)\n",
    "    'rnn_size': 128,   # number of LSTM cells of each layer (128/256 recommended)\n",
    "    'rnn_layers': 3,   # number of LSTM layers (>=2 recommended)\n",
    "    'rnn_bidirectional': False,   # consider text both forwards and backward, can give a training boost\n",
    "    'max_length': 30,   # number of tokens to consider before predicting the next (20-40 for characters, 5-10 for words recommended)\n",
    "    'max_words': 10000,   # maximum number of words to model; the rest will be ignored (word-level model only)\n",
    "}\n",
    "\n",
    "train_cfg = {\n",
    "    'line_delimited': False,   # set to True if each text has its own line in the source file\n",
    "    'num_epochs': 50,   # set higher to train the model for longer\n",
    "    'gen_epochs': 125,   # generates sample text from model after given number of epochs\n",
    "    'train_size': 0.8,   # proportion of input data to train on: setting < 1.0 limits model from learning perfectly\n",
    "    'dropout': 0.1,   # ignore a random proportion of source tokens each epoch, allowing model to generalize better\n",
    "    'validation': False,   # If train__size < 1.0, test on holdout dataset; will make overall training slower\n",
    "    'is_csv': False   # set to True if file is a CSV exported from Excel/BigQuery/pandas\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'body_type', 'diet', 'drinks', 'drugs', 'education', 'essay0',\n",
       "       'essay1', 'essay2', 'essay3',\n",
       "       ...\n",
       "       'w199', 'w200', 'sd', 'ave_sentiment', 'Flesch_Kincaid',\n",
       "       'Gunning_Fog_Index', 'Coleman_Liau', 'SMOG',\n",
       "       'Automated_Readability_Index', 'Average_Grade_Level'],\n",
       "      dtype='object', length=556)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pd.read_csv('okcupid_text.csv')\n",
    "raw_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['body_type', 'diet', 'drinks', 'drugs', 'education', 'essay0', 'essay1',\n",
       "       'essay2', 'essay3', 'essay4', 'essay5', 'essay6', 'essay7', 'essay8',\n",
       "       'essay9', 'ethnicity', 'height', 'income', 'job', 'last_online',\n",
       "       'location', 'offspring', 'orientation', 'pets', 'religion', 'sex',\n",
       "       'sign', 'smokes', 'speaks', 'status', 'uuid', 'age_group', 'body_fat',\n",
       "       'body_fit', 'body_type_noanswer', 'body_thin', 'vegetarian',\n",
       "       'halal_kosher', 'alcohol', 'drugs_rec', 'education_rec', 'grad_school',\n",
       "       'black', 'asian', 'short', 'income_rec', 'haskids', 'wantskids', 'gay'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.columns[1:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay0</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>9995</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>hi!</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3</td>\n",
       "      <td>5946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay0    sex\n",
       "count   10000  10000\n",
       "unique   9995      2\n",
       "top       hi!      m\n",
       "freq        3   5946"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= raw_df[['essay0', 'sex']]\n",
    "df= df.dropna()\n",
    "df= df.sample(10000)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Text Generation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.essay0 = df.essay0.str.replace('<[^<]+?>', '') \n",
    "df.essay0= df.essay0.str.replace('\\n', '') \n",
    "#essay0[1:10000].to_csv('essay0.csv')\n",
    "#essay0[1:10000]\n",
    "df.essay0\n",
    "df_m = df[df.sex=='m']\n",
    "df_f = df[df.sex=='f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_m =  df_m['essay0'].tolist()\n",
    "text_f =  df_f['essay0'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'okcupid'\n",
    "textgen_f = textgenrnn(name=model_name)\n",
    "textgen_f.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training new model w/ 3-layer, 128-cell LSTMs\n",
      "Training on 483,254 word sequences.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 966 steps\n",
      "Epoch 1/50\n",
      "966/966 [==============================] - 80s 83ms/step - loss: 5.0487\n",
      "Epoch 2/50\n",
      "966/966 [==============================] - 76s 79ms/step - loss: 4.2071\n",
      "Epoch 3/50\n",
      "966/966 [==============================] - 75s 78ms/step - loss: 3.8364\n",
      "Epoch 4/50\n",
      "966/966 [==============================] - 76s 79ms/step - loss: 3.5526\n",
      "Epoch 5/50\n",
      "966/966 [==============================] - 78s 80ms/step - loss: 3.3067\n",
      "Epoch 6/50\n",
      "966/966 [==============================] - 77s 80ms/step - loss: 3.0952\n",
      "Epoch 7/50\n",
      "966/966 [==============================] - 78s 80ms/step - loss: 2.9190\n",
      "Epoch 8/50\n",
      "966/966 [==============================] - 78s 81ms/step - loss: 2.7741\n",
      "Epoch 9/50\n",
      "966/966 [==============================] - 77s 80ms/step - loss: 2.6487\n",
      "Epoch 10/50\n",
      "966/966 [==============================] - 78s 81ms/step - loss: 2.5496\n",
      "Epoch 11/50\n",
      "966/966 [==============================] - 76s 79ms/step - loss: 2.4608\n",
      "Epoch 12/50\n",
      "966/966 [==============================] - 77s 80ms/step - loss: 2.3929\n",
      "Epoch 13/50\n",
      "966/966 [==============================] - 81s 84ms/step - loss: 2.3226\n",
      "Epoch 14/50\n",
      "966/966 [==============================] - 75s 78ms/step - loss: 2.2653\n",
      "Epoch 15/50\n",
      "966/966 [==============================] - 75s 77ms/step - loss: 2.2149\n",
      "Epoch 16/50\n",
      "966/966 [==============================] - 76s 78ms/step - loss: 2.1623\n",
      "Epoch 17/50\n",
      "966/966 [==============================] - 75s 78ms/step - loss: 2.1199\n",
      "Epoch 18/50\n",
      "966/966 [==============================] - 75s 78ms/step - loss: 2.0829\n",
      "Epoch 19/50\n",
      "966/966 [==============================] - 76s 79ms/step - loss: 2.0295\n",
      "Epoch 20/50\n",
      "966/966 [==============================] - 75s 78ms/step - loss: 1.9897\n",
      "Epoch 21/50\n",
      "966/966 [==============================] - 80s 83ms/step - loss: 1.9563\n",
      "Epoch 22/50\n",
      "966/966 [==============================] - 78s 81ms/step - loss: 1.9264\n",
      "Epoch 23/50\n",
      "966/966 [==============================] - 77s 80ms/step - loss: 1.8994\n",
      "Epoch 24/50\n",
      "966/966 [==============================] - 77s 80ms/step - loss: 1.8659\n",
      "Epoch 25/50\n",
      "966/966 [==============================] - 80s 83ms/step - loss: 1.8411\n",
      "Epoch 26/50\n",
      "966/966 [==============================] - 77s 80ms/step - loss: 1.8163\n",
      "Epoch 27/50\n",
      "966/966 [==============================] - 77s 79ms/step - loss: 1.7877\n",
      "Epoch 28/50\n",
      "966/966 [==============================] - 77s 80ms/step - loss: 1.7677\n",
      "Epoch 29/50\n",
      "966/966 [==============================] - 76s 79ms/step - loss: 1.7395\n",
      "Epoch 30/50\n",
      "966/966 [==============================] - 76s 79ms/step - loss: 1.7157\n",
      "Epoch 31/50\n",
      "966/966 [==============================] - 74s 76ms/step - loss: 1.6935\n",
      "Epoch 32/50\n",
      "966/966 [==============================] - 74s 76ms/step - loss: 1.6721\n",
      "Epoch 33/50\n",
      "966/966 [==============================] - 74s 77ms/step - loss: 1.6531\n",
      "Epoch 34/50\n",
      "966/966 [==============================] - 74s 76ms/step - loss: 1.6306\n",
      "Epoch 35/50\n",
      "966/966 [==============================] - 74s 76ms/step - loss: 1.6112\n",
      "Epoch 36/50\n",
      "966/966 [==============================] - 73s 76ms/step - loss: 1.5920\n",
      "Epoch 37/50\n",
      "966/966 [==============================] - 76s 79ms/step - loss: 1.5684\n",
      "Epoch 38/50\n",
      "966/966 [==============================] - 73s 76ms/step - loss: 1.5451\n",
      "Epoch 39/50\n",
      "966/966 [==============================] - 73s 76ms/step - loss: 1.5285\n",
      "Epoch 40/50\n",
      "966/966 [==============================] - 73s 76ms/step - loss: 1.5139\n",
      "Epoch 41/50\n",
      "966/966 [==============================] - 73s 76ms/step - loss: 1.4981\n",
      "Epoch 42/50\n",
      "966/966 [==============================] - 73s 75ms/step - loss: 1.4797\n",
      "Epoch 43/50\n",
      "966/966 [==============================] - 78s 81ms/step - loss: 1.4700\n",
      "Epoch 44/50\n",
      "966/966 [==============================] - 75s 77ms/step - loss: 1.4486\n",
      "Epoch 45/50\n",
      "966/966 [==============================] - 76s 78ms/step - loss: 1.4359\n",
      "Epoch 46/50\n",
      "966/966 [==============================] - 74s 77ms/step - loss: 1.4241\n",
      "Epoch 47/50\n",
      "966/966 [==============================] - 74s 77ms/step - loss: 1.4125\n",
      "Epoch 48/50\n",
      "966/966 [==============================] - 75s 77ms/step - loss: 1.3983\n",
      "Epoch 49/50\n",
      "966/966 [==============================] - 74s 77ms/step - loss: 1.3819\n",
      "Epoch 50/50\n",
      "966/966 [==============================] - ETA: 0s - loss: 1.375 - 75s 78ms/step - loss: 1.3756\n"
     ]
    }
   ],
   "source": [
    "#train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
    "textgen_f.train_new_model( \n",
    "    text_f,\n",
    "    #context_labels=context_labels, \n",
    "    #context=True,\n",
    "    num_epochs=train_cfg['num_epochs'],\n",
    "    gen_epochs=train_cfg['gen_epochs'],\n",
    "    batch_size=500,\n",
    "    train_size=train_cfg['train_size'],\n",
    "    dropout=train_cfg['dropout'],\n",
    "    validation=train_cfg['validation'],\n",
    "    is_csv=train_cfg['is_csv'],\n",
    "    rnn_layers=model_cfg['rnn_layers'],\n",
    "    rnn_size=model_cfg['rnn_size'],\n",
    "    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
    "    max_length=model_cfg['max_length'],\n",
    "    dim_embeddings= 100,\n",
    "    word_level=model_cfg['word_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training new model w/ 3-layer, 128-cell LSTMs\n",
      "Training on 649,374 word sequences.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1298 steps\n",
      "Epoch 1/50\n",
      "1298/1298 [==============================] - 112s 86ms/step - loss: 5.1399\n",
      "Epoch 2/50\n",
      "1298/1298 [==============================] - 107s 83ms/step - loss: 4.4094\n",
      "Epoch 3/50\n",
      "1298/1298 [==============================] - 108s 83ms/step - loss: 4.1194\n",
      "Epoch 4/50\n",
      "1298/1298 [==============================] - 107s 82ms/step - loss: 3.8971\n",
      "Epoch 5/50\n",
      "1298/1298 [==============================] - 107s 82ms/step - loss: 3.7007\n",
      "Epoch 6/50\n",
      "1298/1298 [==============================] - 106s 82ms/step - loss: 3.5159\n",
      "Epoch 7/50\n",
      "1298/1298 [==============================] - 105s 81ms/step - loss: 3.3409\n",
      "Epoch 8/50\n",
      "1298/1298 [==============================] - 105s 81ms/step - loss: 3.1833\n",
      "Epoch 9/50\n",
      "1298/1298 [==============================] - 104s 80ms/step - loss: 3.0610\n",
      "Epoch 10/50\n",
      "1298/1298 [==============================] - 105s 81ms/step - loss: 2.9411\n",
      "Epoch 11/50\n",
      "1298/1298 [==============================] - 105s 81ms/step - loss: 2.8409\n",
      "Epoch 12/50\n",
      "1298/1298 [==============================] - 105s 81ms/step - loss: 2.7602\n",
      "Epoch 13/50\n",
      "1298/1298 [==============================] - 105s 81ms/step - loss: 2.6863\n",
      "Epoch 14/50\n",
      "1298/1298 [==============================] - 106s 82ms/step - loss: 2.6257\n",
      "Epoch 15/50\n",
      "1298/1298 [==============================] - 105s 81ms/step - loss: 2.5675\n",
      "Epoch 16/50\n",
      "1298/1298 [==============================] - 105s 81ms/step - loss: 2.5176\n",
      "Epoch 17/50\n",
      "1298/1298 [==============================] - 106s 82ms/step - loss: 2.4592\n",
      "Epoch 18/50\n",
      "1298/1298 [==============================] - 105s 81ms/step - loss: 2.4115\n",
      "Epoch 19/50\n",
      "1298/1298 [==============================] - 104s 80ms/step - loss: 2.3723\n",
      "Epoch 20/50\n",
      "1298/1298 [==============================] - 105s 81ms/step - loss: 2.3360\n",
      "Epoch 21/50\n",
      "1298/1298 [==============================] - 104s 80ms/step - loss: 2.3093\n",
      "Epoch 22/50\n",
      "1298/1298 [==============================] - 105s 81ms/step - loss: 2.2682\n",
      "Epoch 23/50\n",
      "1298/1298 [==============================] - 105s 81ms/step - loss: 2.2447\n",
      "Epoch 24/50\n",
      "1298/1298 [==============================] - 106s 81ms/step - loss: 2.2136\n",
      "Epoch 25/50\n",
      "1298/1298 [==============================] - 105s 81ms/step - loss: 2.1872\n",
      "Epoch 26/50\n",
      "1298/1298 [==============================] - 104s 80ms/step - loss: 2.1615\n",
      "Epoch 27/50\n",
      "1298/1298 [==============================] - 105s 81ms/step - loss: 2.1368\n",
      "Epoch 28/50\n",
      "1298/1298 [==============================] - 104s 80ms/step - loss: 2.1105\n",
      "Epoch 29/50\n",
      "1298/1298 [==============================] - 106s 81ms/step - loss: 2.0929\n",
      "Epoch 30/50\n",
      "1298/1298 [==============================] - 104s 80ms/step - loss: 2.0655\n",
      "Epoch 31/50\n",
      "1298/1298 [==============================] - 106s 82ms/step - loss: 2.0423\n",
      "Epoch 32/50\n",
      "1298/1298 [==============================] - 104s 81ms/step - loss: 2.0210\n",
      "Epoch 33/50\n",
      "1298/1298 [==============================] - 107s 82ms/step - loss: 1.9981\n",
      "Epoch 34/50\n",
      "1298/1298 [==============================] - 104s 80ms/step - loss: 1.9708\n",
      "Epoch 35/50\n",
      "1298/1298 [==============================] - 104s 80ms/step - loss: 1.9534\n",
      "Epoch 36/50\n",
      "1298/1298 [==============================] - 105s 81ms/step - loss: 1.9383\n",
      "Epoch 37/50\n",
      "1298/1298 [==============================] - 111s 85ms/step - loss: 1.9184\n",
      "Epoch 38/50\n",
      "1298/1298 [==============================] - 108s 83ms/step - loss: 1.9021\n",
      "Epoch 39/50\n",
      "1298/1298 [==============================] - 109s 84ms/step - loss: 1.8884\n",
      "Epoch 40/50\n",
      "1298/1298 [==============================] - 112s 86ms/step - loss: 1.8671\n",
      "Epoch 41/50\n",
      "1298/1298 [==============================] - 111s 86ms/step - loss: 1.8573\n",
      "Epoch 42/50\n",
      "1298/1298 [==============================] - 113s 87ms/step - loss: 1.8394\n",
      "Epoch 43/50\n",
      "1298/1298 [==============================] - 130s 100ms/step - loss: 1.8231\n",
      "Epoch 44/50\n",
      "1298/1298 [==============================] - 114s 88ms/step - loss: 1.8065\n",
      "Epoch 45/50\n",
      "1298/1298 [==============================] - 111s 85ms/step - loss: 1.7960\n",
      "Epoch 46/50\n",
      "1298/1298 [==============================] - 108s 83ms/step - loss: 1.7806\n",
      "Epoch 47/50\n",
      "1298/1298 [==============================] - 107s 83ms/step - loss: 1.7663\n",
      "Epoch 48/50\n",
      "1298/1298 [==============================] - 114s 88ms/step - loss: 1.7539\n",
      "Epoch 49/50\n",
      "1298/1298 [==============================] - 117s 90ms/step - loss: 1.7434\n",
      "Epoch 50/50\n",
      "1298/1298 [==============================] - 108s 83ms/step - loss: 1.7291\n"
     ]
    }
   ],
   "source": [
    "textgen_m = textgenrnn(name=model_name)\n",
    "textgen_m.train_new_model( \n",
    "    text_m,\n",
    "    #ontext_labels=context_labels, \n",
    "    #context=True,\n",
    "    num_epochs=train_cfg['num_epochs'],\n",
    "    gen_epochs=train_cfg['gen_epochs'],\n",
    "    batch_size=500,\n",
    "    train_size=train_cfg['train_size'],\n",
    "    dropout=train_cfg['dropout'],\n",
    "    validation=train_cfg['validation'],\n",
    "    is_csv=train_cfg['is_csv'],\n",
    "    rnn_layers=model_cfg['rnn_layers'],\n",
    "    rnn_size=model_cfg['rnn_size'],\n",
    "    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
    "    max_length=model_cfg['max_length'],\n",
    "    dim_embeddings= 100,\n",
    "    word_level=model_cfg['word_level'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate profile intro for different gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Male "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████                                                        | 1/3 [00:00<00:01,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm a smart, financially - stable - athletic type of guy who's his journey as about it has the way.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|████████████████████████████████████████████████████████                            | 2/3 [00:01<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm a college professor and working out in san francisco, but sometimes also i enjoy it.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:07<00:00,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm a pretty easy going, laid back guy. i'm a pretty outgoing guy. i'm star wars or attitude. i used to be a spanish - year in japan and i enjoy the world. i moved to san francisco ago from a year ago. i am prettymuch a major of city life to the city. i love this city life, enjoy meeting new people and quality. i'm always looking for new people or gay, hanging out with a bar or drinks food, wine and listening to good wine. i am a calm person, but i also like to keep fit, and am always looking for good people to make new people. i'm a rebel at heart and gets pretty good at times. i have a lot of acquaintances but am i'm very passionate and a career at my own, but i'm not sure to be pretty good at. finally, i know i look out, however, it doesn't really get to know, although all i'm not saying.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "textgen_m.generate(3, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████                                                        | 1/3 [00:02<00:05,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am a lover of mustaches,, take walks of the city, and like a variety of bands in the us. the best is: i tend to enjoy a walk around the closer to the beach. i believe everyone has a purpose. i believe that in relationship and experience is the most is a turn starting to laugh and i would like to be. i'm very well. i am strong, loving, and caring\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|████████████████████████████████████████████████████████                            | 2/3 [00:08<00:03,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i'm an east coast transplant who's cute, fun, fun lives looking for family andfriends and learning. i like to go to the movies and have been active to get to dinner parties where other people came of the country. favorite food, wine, food, wine, food, wine, and food. i'd like to meet a man who knows how to ground and life. i am very honest, and mostly just comfortable with a good movie and there. my last relationship traveling and being out of the country. especially in urban church. i'm curious about a bit of a project that it's 4 - but i'm pretty sure the most't in this summer. in case, i'm a short person. i'm not in a car and soccer field. in the past i like being outdoors, camping, hiking, biking, swimming, shopping, playingtennis and and seeing movies.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:10<00:00,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i've lived in the bay area for almost a couple years now. i work as a tech - class; working in a used city, went an avid rock band geek, where's going to change for school. i love to get out more, whether it's all talking to just do where i am from the bay. i will never say where i am doing.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "textgen_f.generate(3, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
